Huffman coding achieves optimality through **frequency-weighted binary tree construction**. The compression ratio depends on Shannon entropy (fundamental limit) and the actual code lengths.

### Key Formula
**Compression Ratio**
\[
R = \frac{H(S)}{L}
\]
Where:
- \( H(S) = -\sum_{i=1}^n p_i \log_2 p_i \) (Shannon entropy)
- \( L = \sum_{i=1}^n p_i l_i \) (Average code length)
- \( p_i \) = Probability of symbol \( i \)
- \( l_i \) = Code length for symbol \( i \)

---

### Optimization Process Table
| Step | Action | Mathematical Guarantee |
|------|--------|-------------------------|
| 1 | Sort symbols by frequency | \( p_1 \geq p_2 \geq ... \geq p_n \) |
| 2 | Build priority queue | \( O(n \log n) \) time complexity |
| 3 | Merge lowest-frequency nodes | \( l_i \propto -\log_2 p_i \) |
| 4 | Assign 0/1 to tree branches | Prefix-free condition: \( \forall i,j,\ c_i \not\subseteq c_j \) |

---

**Concrete Example**
For alphabet {A, B, C, D} with frequencies:

| Symbol | Frequency | Huffman Code | Original Bits | Compressed Bits |
|--------|-----------|--------------|---------------|-----------------|
| A      | 0.4       | 0            | 8             | 3.2             |
| B      | 0.3       | 10           | 8             | 2.4             |
| C      | 0.2       | 110          | 8             | 1.6             |
| D      | 0.1       | 111          | 8             | 0.8             |

**Total**:
Original = 32 bits (4 symbols Ã— 8-bit ASCII)
Compressed = 8 bits (sum of Compressed Bits column)
**Compression Ratio** = 32/8 = **4:1**

---

**Technical Notes**
1. Huffman codes satisfy:
\[
H(S) \leq L < H(S) + 1
\]
2. Optimality proof via Kraft-McMillan inequality
3. Practical implementations use canonical Huffman codes for decoder efficiency

This problem avoids subjective debates while demonstrating fundamental information theory concepts with verifiable mathematics.
