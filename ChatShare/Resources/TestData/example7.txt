### 7. **优化算法：梯度下降法的变种对比**
**问题**：比较动量法和Adam优化器的更新规则。
**答案**：
1. **动量法**：$v_{t} = \gamma v_{t-1} + \eta \nabla J(\theta_t)$，$\theta_{t+1} = \theta_t - v_t$；
2. **Adam**：结合动量与自适应学习率，引入一阶和二阶矩估计；
3. **代码**（PyTorch示例）：
```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
```
